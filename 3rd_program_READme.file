Combine computer vision and natural language processing to build an image captioning AI. Use pre-trained image recognition models like VGG or ResNet to extract features from images, and then use a
recurrent neural network (RNN) or transformer-based model to generate captions for those images.

#program code 
import tensorflow as tf
from tensorflow.keras.applications import ResNet50
from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions
from tensorflow.keras.preprocessing import image
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, LSTM, Embedding, Dense

import numpy as np
from PIL import Image

# Load pre-trained ResNet50 model for image feature extraction
base_model = ResNet50(weights='imagenet')
image_model = Model(inputs=base_model.input, outputs=base_model.get_layer('avg_pool').output)

# Load pre-trained tokenizer for text processing
tokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token="<OOV>")
tokenizer.fit_on_texts(["<OOV>"])

# Define the LSTM model for caption generation
embedding_dim = 256
units = 512
vocab_size = len(tokenizer.word_index) + 1

inputs = Input(shape=(None,))
embedding = Embedding(vocab_size, embedding_dim, mask_zero=True)(inputs)
lstm = LSTM(units, return_sequences=True, recurrent_initializer='glorot_uniform')(embedding)
dense = Dense(vocab_size)(lstm)

caption_model = Model(inputs=inputs, outputs=dense)

# Function to preprocess the image and extract features
def preprocess_image(img_path):
    img = image.load_img(img_path, target_size=(224, 224))
    img_array = image.img_to_array(img)
    img_array = np.expand_dims(img_array, axis=0)
    img_array = preprocess_input(img_array)
    return img_array

# Function to generate captions for the image
def generate_caption(img_path):
    img_array = preprocess_image(img_path)
    features = image_model.predict(img_array)

    input_caption = "<start>"
    input_sequence = tokenizer.texts_to_sequences([input_caption])[0]
    input_sequence = tf.keras.preprocessing.sequence.pad_sequences([input_sequence], padding='post')

    result_caption = []
    for i in range(20):  # Maximum caption length
        predictions = caption_model.predict(input_sequence)
        predicted_id = np.argmax(predictions[:, i, :])
        if predicted_id == 0:  # End of the caption
            break
        result_caption.append(tokenizer.index_word[predicted_id])

        input_sequence = np.concatenate([input_sequence, np.expand_dims([predicted_id], axis=1)], axis=-1)

    return ' '.join(result_caption)

# Example usage
img_path = 'path_to_your_image.jpg'
caption = generate_caption(img_path)
print(f"Image Caption: {caption}")
